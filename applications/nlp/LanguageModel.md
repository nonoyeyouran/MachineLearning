- [预训练语言模型PLM](#预训练语言模型PLM) <br/>

语言模型文档
## 预训练语言模型PLM
### Bert
bert是google提出的一个预训练语言模型，以transformer的encoder结构为基础构建。<br/>
学习文档：https://zhuanlan.zhihu.com/p/46652512 <br/>
bert主要用于一些判别性任务，不能用于生成。
## 大语言模型（LLM）
### 1. 大语言模型（开源和闭源） 
### 2. 大语言模型架构
### 3. 大语言模型训练
### 4. 大语言模型微调和对齐。
    大模型微调是指将基础训练好的通用大模型在具体的下游任务领域的数据上进一步训练，以适应领域的知识，从而在特定领域表现更佳。
    大模型对齐是指将模型的输出进行限制，通常要符合人类规范（法律、道德等）或领域规范。
    大语言模型微调包括指令微调、propmt tuning、prefix tuning等<br/>
#### 4.1 微调（tuning）
##### 4.1.1 指令微调
    指令微调通常是要大模型学习具体[指令，输出]的方式来限定模型的行为符合人类指令或特定领域指令。
#### 4.2 对齐（alignent）
### 5. 大语言模型应用
