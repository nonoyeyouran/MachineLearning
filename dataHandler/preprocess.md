# 机器学习中的数据预处理

## 目录
- [1. 缺失值处理](#1-缺失值处理)
  - [删除法](#删除法)
  - [填补法（插补）](#填补法插补)
  - [标记缺失值](#标记缺失值)
- [2. 默认值设置](#2-默认值设置)
- [3. 数值化](#3-数值化)
  - [标签编码（Label Encoding）](#标签编码label-encoding)
  - [独热编码（One-Hot Encoding）](#独热编码one-hot-encoding)
  - [特征缩放（Scaling）](#特征缩放scaling)
  - [文本向量化](#文本向量化)
- [4. 离散化](#4-离散化)
  - [为什么要离散化？](#为什么要离散化)
  - [离散化方法](#离散化方法)
  - [实现步骤](#实现步骤)
  - [注意事项](#注意事项)
- [5. 其他预处理技术](#5-其他预处理技术)
- [6. 完整预处理流程](#6-完整预处理流程)
  - [示例](#示例)
- [7. 总结](#7-总结)

在机器学习中，数据预处理是确保模型性能的关键步骤。原始数据往往存在缺失值、噪声或格式不一致等问题，需要通过清洗和转换使其适合模型训练。本文详细介绍数据预处理的常见技术，包括缺失值处理、默认值设置、数值化和离散化。

---

## 1. 缺失值处理

缺失值是指数据集中某些特征值为空（如 NaN、NULL），若不处理，可能导致模型训练失败或结果偏差。处理方法包括：

### 删除法
- **删除行**：若缺失值较少且随机分布，可删除包含缺失值的样本。
  - 适用：数据量较大。
  - 缺点：可能丢失信息。
- **删除列**：若某特征缺失值比例过高（如 >70%），可删除该特征。
  - 优点：简单；缺点：可能丢弃有用信息。

### 填补法（插补）
- **均值/中位数/众数填补**：用特征的统计值填充。
  - 数值型：均值或中位数；类别型：众数。
  - 适用：数据分布均匀。
  - 缺点：可能掩盖变异性。
- **固定值填补**：用预定义值（如 0 或“未知”）填充。
  - 适用：缺失值有特定含义。
- **插值法**：对时间序列数据使用线性插值或高阶插值。
- **模型预测填补**：用其他特征训练模型（如 KNN）预测缺失值。
  - 优点：精确；缺点：计算成本高。

### 标记缺失值
- 将缺失值标记为独立类别（如“Missing”），让模型学习其模式。

---

## 2. 默认值设置

默认值是为某些字段设置的固定值，用于表示特定含义或避免空值。

- **数值型特征**：
  - 示例：缺失年龄设为 0 或平均值。
  - 注意：避免与真实数据混淆。
- **类别型特征**：
  - 示例：缺失性别设为“未知”。
  - 适用：分类模型可将其视为独立类别。
- **注意事项**：
  - 默认值需符合领域知识，避免引入不合理假设。

---

## 3. 数值化

机器学习模型通常要求数值输入，因此需将非数值数据转换为数值形式。

### 标签编码（Label Encoding）
- 将类别映射为整数，如“红”、“蓝”、“绿”编码为 0、1、2。
- 适用：有序类别或树形模型。
- 缺点：无序类别可能引入虚假顺序。

### 独热编码（One-Hot Encoding）
- 为每个类别创建二进制列，如“红”、“蓝”、“绿”转为 [1,0,0]、[0,1,0]、[0,0,1]。
- 适用：无序类别，适合线性模型、神经网络。
- 缺点：类别过多时维度爆炸。

### 特征缩放（Scaling）
- **标准化（Standardization）**：转换为均值 0、标准差 1 的分布，公式：\( x' = (x - \mu) / \sigma \)。
  - 适用：假设正态分布的模型（如 SVM）。
- **归一化（Normalization）**：缩放到 [0, 1]，公式：\( x' = (x - x_{min}) / (x_{max} - x_{min}) \)。
  - 适用：基于距离的模型（如 KNN）。
- 注意：测试集需用训练集参数转换。

### 文本向量化
- 使用词袋模型、TF-IDF 或词嵌入（如 Word2Vec）将文本转为数值向量。
- 适用：NLP 任务。

---

## 4. 离散化

离散化将连续型特征转化为离散区间，简化数据并提高模型适应性。

### 为什么要离散化？
- 提高性能：适合概率模型。
- 处理非线性：将复杂关系简化为分段线性。
- 减少异常值影响：限制异常值到某区间。
- 增强可解释性：结果更直观。

### 离散化方法
1. **等宽分箱（Equal-Width Binning）**
   - 将取值范围分成等宽区间。
   - 示例：年龄 0-100 分成 [0-20), [20-40), [40-60), [60-80), [80-100]。
   - 优点：简单；缺点：对分布不敏感。
2. **等频分箱（Equal-Frequency Binning）**
   - 按样本数量均等分箱。
   - 示例：100 个样本分成 5 箱，每箱 20 个。
   - 优点：分布均衡；缺点：边界不直观。
3. **自定义分箱（Custom Binning）**
   - 手动指定边界，如收入分为“低 (<30万)”、“中 (30-100万)”、“高 (>100万)”。
   - 优点：符合业务逻辑；缺点：依赖专家知识。
4. **基于聚类的分箱**
   - 用聚类算法（如 K-means）分箱。
   - 示例：身高分为“矮”、“中等”、“高”。
   - 优点：适应分布；缺点：计算复杂。
5. **监督离散化**
   - 结合目标变量分箱，如基于熵的分箱。
   - 适用：分类任务。

### 实现步骤
1. 确定分箱方法和数量。
2. 计算分箱边界。
3. 将数据映射到区间，用整数或标签表示。
4. 可选：编码离散化结果。

### 注意事项
- 分箱数量需平衡信息保留与噪声控制。
- 处理超出范围的值（如归入最近箱）。
- 与模型匹配：树形模型对离散化不敏感，距离模型需谨慎。

---

## 5. 其他预处理技术
- **异常值处理**：用 IQR 或 Z 分数移除异常值。
- **特征选择**：剔除冗余特征。
- **数据平衡**：通过过采样（如 SMOTE）或欠采样调整类别分布。

---

## 6. 完整预处理流程
1. **缺失值处理**：填补或删除。
2. **默认值设置**：指定默认值。
3. **离散化**：将连续特征分箱。
4. **数值化**：编码类别特征，缩放数值特征。
5. **其他步骤**：异常值处理、特征选择等。

### 示例
- 原始数据：年龄 [23, 45, 67, NaN]，性别 [男, 女, NaN, 男]。
- 缺失值处理：年龄 NaN 填 45，性别 NaN 填“未知”。
- 离散化：年龄分箱为 [0-30), [30-60), [60-100)，结果 [0, 1, 2, 1]。
- 数值化：性别独热编码为 [1,0,0], [0,1,0], [0,0,1], [1,0,0]。

---

## 7. 总结
数据预处理将原始数据转化为适合模型的格式。缺失值处理确保完整性，默认值提供替代方案，数值化保证兼容性，离散化简化连续数据。方法选择需结合数据特性、任务目标和模型类型，通过实验优化策略。
